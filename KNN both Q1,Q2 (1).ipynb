{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18d1f64",
   "metadata": {},
   "source": [
    "# Q1).Implement a KNN model to classify the animals in to categorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f66a80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppresses warning messages while you to run your code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60925e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed674699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pandas and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008211b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal name</th>\n",
       "      <th>hair</th>\n",
       "      <th>feathers</th>\n",
       "      <th>eggs</th>\n",
       "      <th>milk</th>\n",
       "      <th>airborne</th>\n",
       "      <th>aquatic</th>\n",
       "      <th>predator</th>\n",
       "      <th>toothed</th>\n",
       "      <th>backbone</th>\n",
       "      <th>breathes</th>\n",
       "      <th>venomous</th>\n",
       "      <th>fins</th>\n",
       "      <th>legs</th>\n",
       "      <th>tail</th>\n",
       "      <th>domestic</th>\n",
       "      <th>catsize</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antelope</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bass</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bear</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boar</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  animal name  hair  feathers  eggs  milk  airborne  aquatic  predator  \\\n",
       "0    aardvark     1         0     0     1         0        0         1   \n",
       "1    antelope     1         0     0     1         0        0         0   \n",
       "2        bass     0         0     1     0         0        1         1   \n",
       "3        bear     1         0     0     1         0        0         1   \n",
       "4        boar     1         0     0     1         0        0         1   \n",
       "\n",
       "   toothed  backbone  breathes  venomous  fins  legs  tail  domestic  catsize  \\\n",
       "0        1         1         1         0     0     4     0         0        1   \n",
       "1        1         1         1         0     0     4     1         0        1   \n",
       "2        1         1         0         0     1     0     1         0        0   \n",
       "3        1         1         1         0     0     4     0         0        1   \n",
       "4        1         1         1         0     0     4     1         0        1   \n",
       "\n",
       "   type  \n",
       "0     1  \n",
       "1     1  \n",
       "2     4  \n",
       "3     1  \n",
       "4     1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Zoo.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c9e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b84a7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hair</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.425743</td>\n",
       "      <td>0.496921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feathers</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.400495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.584158</td>\n",
       "      <td>0.495325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milk</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.405941</td>\n",
       "      <td>0.493522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airborne</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.427750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aquatic</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.356436</td>\n",
       "      <td>0.481335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predator</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>0.499505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toothed</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.603960</td>\n",
       "      <td>0.491512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backbone</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>0.384605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breathes</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.792079</td>\n",
       "      <td>0.407844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>venomous</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.271410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fins</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.376013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legs</th>\n",
       "      <td>101.0</td>\n",
       "      <td>2.841584</td>\n",
       "      <td>2.033385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tail</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.742574</td>\n",
       "      <td>0.439397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domestic</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.336552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catsize</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.435644</td>\n",
       "      <td>0.498314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>101.0</td>\n",
       "      <td>2.831683</td>\n",
       "      <td>2.102709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count      mean       std  min  25%  50%  75%  max\n",
       "hair      101.0  0.425743  0.496921  0.0  0.0  0.0  1.0  1.0\n",
       "feathers  101.0  0.198020  0.400495  0.0  0.0  0.0  0.0  1.0\n",
       "eggs      101.0  0.584158  0.495325  0.0  0.0  1.0  1.0  1.0\n",
       "milk      101.0  0.405941  0.493522  0.0  0.0  0.0  1.0  1.0\n",
       "airborne  101.0  0.237624  0.427750  0.0  0.0  0.0  0.0  1.0\n",
       "aquatic   101.0  0.356436  0.481335  0.0  0.0  0.0  1.0  1.0\n",
       "predator  101.0  0.554455  0.499505  0.0  0.0  1.0  1.0  1.0\n",
       "toothed   101.0  0.603960  0.491512  0.0  0.0  1.0  1.0  1.0\n",
       "backbone  101.0  0.821782  0.384605  0.0  1.0  1.0  1.0  1.0\n",
       "breathes  101.0  0.792079  0.407844  0.0  1.0  1.0  1.0  1.0\n",
       "venomous  101.0  0.079208  0.271410  0.0  0.0  0.0  0.0  1.0\n",
       "fins      101.0  0.168317  0.376013  0.0  0.0  0.0  0.0  1.0\n",
       "legs      101.0  2.841584  2.033385  0.0  2.0  4.0  4.0  8.0\n",
       "tail      101.0  0.742574  0.439397  0.0  0.0  1.0  1.0  1.0\n",
       "domestic  101.0  0.128713  0.336552  0.0  0.0  0.0  0.0  1.0\n",
       "catsize   101.0  0.435644  0.498314  0.0  0.0  0.0  1.0  1.0\n",
       "type      101.0  2.831683  2.102709  1.0  1.0  2.0  4.0  7.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6171ff17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hair</th>\n",
       "      <th>feathers</th>\n",
       "      <th>eggs</th>\n",
       "      <th>milk</th>\n",
       "      <th>airborne</th>\n",
       "      <th>aquatic</th>\n",
       "      <th>predator</th>\n",
       "      <th>toothed</th>\n",
       "      <th>backbone</th>\n",
       "      <th>breathes</th>\n",
       "      <th>venomous</th>\n",
       "      <th>fins</th>\n",
       "      <th>legs</th>\n",
       "      <th>tail</th>\n",
       "      <th>domestic</th>\n",
       "      <th>catsize</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hair</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.427851</td>\n",
       "      <td>-0.817382</td>\n",
       "      <td>0.878503</td>\n",
       "      <td>-0.198431</td>\n",
       "      <td>-0.473554</td>\n",
       "      <td>-0.154769</td>\n",
       "      <td>0.492531</td>\n",
       "      <td>0.191681</td>\n",
       "      <td>0.441149</td>\n",
       "      <td>-0.104245</td>\n",
       "      <td>-0.280313</td>\n",
       "      <td>0.394009</td>\n",
       "      <td>0.048973</td>\n",
       "      <td>0.207208</td>\n",
       "      <td>0.455020</td>\n",
       "      <td>-0.562384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feathers</th>\n",
       "      <td>-0.427851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419248</td>\n",
       "      <td>-0.410761</td>\n",
       "      <td>0.656553</td>\n",
       "      <td>-0.058552</td>\n",
       "      <td>-0.104430</td>\n",
       "      <td>-0.613631</td>\n",
       "      <td>0.231403</td>\n",
       "      <td>0.254588</td>\n",
       "      <td>-0.145739</td>\n",
       "      <td>-0.223541</td>\n",
       "      <td>-0.206686</td>\n",
       "      <td>0.292569</td>\n",
       "      <td>0.031586</td>\n",
       "      <td>-0.135934</td>\n",
       "      <td>-0.197520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>-0.817382</td>\n",
       "      <td>0.419248</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.938848</td>\n",
       "      <td>0.376646</td>\n",
       "      <td>0.376244</td>\n",
       "      <td>0.011605</td>\n",
       "      <td>-0.642150</td>\n",
       "      <td>-0.340420</td>\n",
       "      <td>-0.382777</td>\n",
       "      <td>0.098689</td>\n",
       "      <td>0.164796</td>\n",
       "      <td>-0.224918</td>\n",
       "      <td>-0.221090</td>\n",
       "      <td>-0.155610</td>\n",
       "      <td>-0.514650</td>\n",
       "      <td>0.661825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milk</th>\n",
       "      <td>0.878503</td>\n",
       "      <td>-0.410761</td>\n",
       "      <td>-0.938848</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.366765</td>\n",
       "      <td>-0.362613</td>\n",
       "      <td>-0.029721</td>\n",
       "      <td>0.628168</td>\n",
       "      <td>0.384958</td>\n",
       "      <td>0.423527</td>\n",
       "      <td>-0.242449</td>\n",
       "      <td>-0.156328</td>\n",
       "      <td>0.214196</td>\n",
       "      <td>0.210026</td>\n",
       "      <td>0.163928</td>\n",
       "      <td>0.574906</td>\n",
       "      <td>-0.723683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airborne</th>\n",
       "      <td>-0.198431</td>\n",
       "      <td>0.656553</td>\n",
       "      <td>0.376646</td>\n",
       "      <td>-0.366765</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.172638</td>\n",
       "      <td>-0.295181</td>\n",
       "      <td>-0.594311</td>\n",
       "      <td>-0.104718</td>\n",
       "      <td>0.286039</td>\n",
       "      <td>0.008528</td>\n",
       "      <td>-0.251157</td>\n",
       "      <td>0.043712</td>\n",
       "      <td>0.009482</td>\n",
       "      <td>0.063274</td>\n",
       "      <td>-0.349768</td>\n",
       "      <td>0.022677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aquatic</th>\n",
       "      <td>-0.473554</td>\n",
       "      <td>-0.058552</td>\n",
       "      <td>0.376244</td>\n",
       "      <td>-0.362613</td>\n",
       "      <td>-0.172638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375978</td>\n",
       "      <td>0.053150</td>\n",
       "      <td>0.022463</td>\n",
       "      <td>-0.637506</td>\n",
       "      <td>0.087915</td>\n",
       "      <td>0.604492</td>\n",
       "      <td>-0.360638</td>\n",
       "      <td>-0.034642</td>\n",
       "      <td>-0.224308</td>\n",
       "      <td>-0.111866</td>\n",
       "      <td>0.326639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predator</th>\n",
       "      <td>-0.154769</td>\n",
       "      <td>-0.104430</td>\n",
       "      <td>0.011605</td>\n",
       "      <td>-0.029721</td>\n",
       "      <td>-0.295181</td>\n",
       "      <td>0.375978</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129452</td>\n",
       "      <td>0.051022</td>\n",
       "      <td>-0.262931</td>\n",
       "      <td>0.115391</td>\n",
       "      <td>0.190302</td>\n",
       "      <td>-0.099723</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>-0.309794</td>\n",
       "      <td>0.144790</td>\n",
       "      <td>0.061179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toothed</th>\n",
       "      <td>0.492531</td>\n",
       "      <td>-0.613631</td>\n",
       "      <td>-0.642150</td>\n",
       "      <td>0.628168</td>\n",
       "      <td>-0.594311</td>\n",
       "      <td>0.053150</td>\n",
       "      <td>0.129452</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.575085</td>\n",
       "      <td>-0.065690</td>\n",
       "      <td>-0.062344</td>\n",
       "      <td>0.364292</td>\n",
       "      <td>-0.193476</td>\n",
       "      <td>0.310368</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>0.344010</td>\n",
       "      <td>-0.471527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backbone</th>\n",
       "      <td>0.191681</td>\n",
       "      <td>0.231403</td>\n",
       "      <td>-0.340420</td>\n",
       "      <td>0.384958</td>\n",
       "      <td>-0.104718</td>\n",
       "      <td>0.022463</td>\n",
       "      <td>0.051022</td>\n",
       "      <td>0.575085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.207666</td>\n",
       "      <td>-0.246611</td>\n",
       "      <td>0.209499</td>\n",
       "      <td>-0.432856</td>\n",
       "      <td>0.731762</td>\n",
       "      <td>0.101733</td>\n",
       "      <td>0.356976</td>\n",
       "      <td>-0.828845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breathes</th>\n",
       "      <td>0.441149</td>\n",
       "      <td>0.254588</td>\n",
       "      <td>-0.382777</td>\n",
       "      <td>0.423527</td>\n",
       "      <td>0.286039</td>\n",
       "      <td>-0.637506</td>\n",
       "      <td>-0.262931</td>\n",
       "      <td>-0.065690</td>\n",
       "      <td>0.207666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.120752</td>\n",
       "      <td>-0.617219</td>\n",
       "      <td>0.369868</td>\n",
       "      <td>0.088952</td>\n",
       "      <td>0.124068</td>\n",
       "      <td>0.204125</td>\n",
       "      <td>-0.519308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>venomous</th>\n",
       "      <td>-0.104245</td>\n",
       "      <td>-0.145739</td>\n",
       "      <td>0.098689</td>\n",
       "      <td>-0.242449</td>\n",
       "      <td>0.008528</td>\n",
       "      <td>0.087915</td>\n",
       "      <td>0.115391</td>\n",
       "      <td>-0.062344</td>\n",
       "      <td>-0.246611</td>\n",
       "      <td>-0.120752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.033956</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>-0.162724</td>\n",
       "      <td>-0.003252</td>\n",
       "      <td>-0.183748</td>\n",
       "      <td>0.321476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fins</th>\n",
       "      <td>-0.280313</td>\n",
       "      <td>-0.223541</td>\n",
       "      <td>0.164796</td>\n",
       "      <td>-0.156328</td>\n",
       "      <td>-0.251157</td>\n",
       "      <td>0.604492</td>\n",
       "      <td>0.190302</td>\n",
       "      <td>0.364292</td>\n",
       "      <td>0.209499</td>\n",
       "      <td>-0.617219</td>\n",
       "      <td>-0.033956</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.605652</td>\n",
       "      <td>0.204349</td>\n",
       "      <td>-0.093887</td>\n",
       "      <td>0.031705</td>\n",
       "      <td>0.099430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legs</th>\n",
       "      <td>0.394009</td>\n",
       "      <td>-0.206686</td>\n",
       "      <td>-0.224918</td>\n",
       "      <td>0.214196</td>\n",
       "      <td>0.043712</td>\n",
       "      <td>-0.360638</td>\n",
       "      <td>-0.099723</td>\n",
       "      <td>-0.193476</td>\n",
       "      <td>-0.432856</td>\n",
       "      <td>0.369868</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>-0.605652</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.348295</td>\n",
       "      <td>0.073931</td>\n",
       "      <td>0.068791</td>\n",
       "      <td>0.131693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tail</th>\n",
       "      <td>0.048973</td>\n",
       "      <td>0.292569</td>\n",
       "      <td>-0.221090</td>\n",
       "      <td>0.210026</td>\n",
       "      <td>0.009482</td>\n",
       "      <td>-0.034642</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>0.310368</td>\n",
       "      <td>0.731762</td>\n",
       "      <td>0.088952</td>\n",
       "      <td>-0.162724</td>\n",
       "      <td>0.204349</td>\n",
       "      <td>-0.348295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023434</td>\n",
       "      <td>0.243277</td>\n",
       "      <td>-0.631830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domestic</th>\n",
       "      <td>0.207208</td>\n",
       "      <td>0.031586</td>\n",
       "      <td>-0.155610</td>\n",
       "      <td>0.163928</td>\n",
       "      <td>0.063274</td>\n",
       "      <td>-0.224308</td>\n",
       "      <td>-0.309794</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>0.101733</td>\n",
       "      <td>0.124068</td>\n",
       "      <td>-0.003252</td>\n",
       "      <td>-0.093887</td>\n",
       "      <td>0.073931</td>\n",
       "      <td>0.023434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>-0.181043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catsize</th>\n",
       "      <td>0.455020</td>\n",
       "      <td>-0.135934</td>\n",
       "      <td>-0.514650</td>\n",
       "      <td>0.574906</td>\n",
       "      <td>-0.349768</td>\n",
       "      <td>-0.111866</td>\n",
       "      <td>0.144790</td>\n",
       "      <td>0.344010</td>\n",
       "      <td>0.356976</td>\n",
       "      <td>0.204125</td>\n",
       "      <td>-0.183748</td>\n",
       "      <td>0.031705</td>\n",
       "      <td>0.068791</td>\n",
       "      <td>0.243277</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.521030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>-0.562384</td>\n",
       "      <td>-0.197520</td>\n",
       "      <td>0.661825</td>\n",
       "      <td>-0.723683</td>\n",
       "      <td>0.022677</td>\n",
       "      <td>0.326639</td>\n",
       "      <td>0.061179</td>\n",
       "      <td>-0.471527</td>\n",
       "      <td>-0.828845</td>\n",
       "      <td>-0.519308</td>\n",
       "      <td>0.321476</td>\n",
       "      <td>0.099430</td>\n",
       "      <td>0.131693</td>\n",
       "      <td>-0.631830</td>\n",
       "      <td>-0.181043</td>\n",
       "      <td>-0.521030</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              hair  feathers      eggs      milk  airborne   aquatic  \\\n",
       "hair      1.000000 -0.427851 -0.817382  0.878503 -0.198431 -0.473554   \n",
       "feathers -0.427851  1.000000  0.419248 -0.410761  0.656553 -0.058552   \n",
       "eggs     -0.817382  0.419248  1.000000 -0.938848  0.376646  0.376244   \n",
       "milk      0.878503 -0.410761 -0.938848  1.000000 -0.366765 -0.362613   \n",
       "airborne -0.198431  0.656553  0.376646 -0.366765  1.000000 -0.172638   \n",
       "aquatic  -0.473554 -0.058552  0.376244 -0.362613 -0.172638  1.000000   \n",
       "predator -0.154769 -0.104430  0.011605 -0.029721 -0.295181  0.375978   \n",
       "toothed   0.492531 -0.613631 -0.642150  0.628168 -0.594311  0.053150   \n",
       "backbone  0.191681  0.231403 -0.340420  0.384958 -0.104718  0.022463   \n",
       "breathes  0.441149  0.254588 -0.382777  0.423527  0.286039 -0.637506   \n",
       "venomous -0.104245 -0.145739  0.098689 -0.242449  0.008528  0.087915   \n",
       "fins     -0.280313 -0.223541  0.164796 -0.156328 -0.251157  0.604492   \n",
       "legs      0.394009 -0.206686 -0.224918  0.214196  0.043712 -0.360638   \n",
       "tail      0.048973  0.292569 -0.221090  0.210026  0.009482 -0.034642   \n",
       "domestic  0.207208  0.031586 -0.155610  0.163928  0.063274 -0.224308   \n",
       "catsize   0.455020 -0.135934 -0.514650  0.574906 -0.349768 -0.111866   \n",
       "type     -0.562384 -0.197520  0.661825 -0.723683  0.022677  0.326639   \n",
       "\n",
       "          predator   toothed  backbone  breathes  venomous      fins  \\\n",
       "hair     -0.154769  0.492531  0.191681  0.441149 -0.104245 -0.280313   \n",
       "feathers -0.104430 -0.613631  0.231403  0.254588 -0.145739 -0.223541   \n",
       "eggs      0.011605 -0.642150 -0.340420 -0.382777  0.098689  0.164796   \n",
       "milk     -0.029721  0.628168  0.384958  0.423527 -0.242449 -0.156328   \n",
       "airborne -0.295181 -0.594311 -0.104718  0.286039  0.008528 -0.251157   \n",
       "aquatic   0.375978  0.053150  0.022463 -0.637506  0.087915  0.604492   \n",
       "predator  1.000000  0.129452  0.051022 -0.262931  0.115391  0.190302   \n",
       "toothed   0.129452  1.000000  0.575085 -0.065690 -0.062344  0.364292   \n",
       "backbone  0.051022  0.575085  1.000000  0.207666 -0.246611  0.209499   \n",
       "breathes -0.262931 -0.065690  0.207666  1.000000 -0.120752 -0.617219   \n",
       "venomous  0.115391 -0.062344 -0.246611 -0.120752  1.000000 -0.033956   \n",
       "fins      0.190302  0.364292  0.209499 -0.617219 -0.033956  1.000000   \n",
       "legs     -0.099723 -0.193476 -0.432856  0.369868  0.022964 -0.605652   \n",
       "tail      0.018947  0.310368  0.731762  0.088952 -0.162724  0.204349   \n",
       "domestic -0.309794  0.069430  0.101733  0.124068 -0.003252 -0.093887   \n",
       "catsize   0.144790  0.344010  0.356976  0.204125 -0.183748  0.031705   \n",
       "type      0.061179 -0.471527 -0.828845 -0.519308  0.321476  0.099430   \n",
       "\n",
       "              legs      tail  domestic   catsize      type  \n",
       "hair      0.394009  0.048973  0.207208  0.455020 -0.562384  \n",
       "feathers -0.206686  0.292569  0.031586 -0.135934 -0.197520  \n",
       "eggs     -0.224918 -0.221090 -0.155610 -0.514650  0.661825  \n",
       "milk      0.214196  0.210026  0.163928  0.574906 -0.723683  \n",
       "airborne  0.043712  0.009482  0.063274 -0.349768  0.022677  \n",
       "aquatic  -0.360638 -0.034642 -0.224308 -0.111866  0.326639  \n",
       "predator -0.099723  0.018947 -0.309794  0.144790  0.061179  \n",
       "toothed  -0.193476  0.310368  0.069430  0.344010 -0.471527  \n",
       "backbone -0.432856  0.731762  0.101733  0.356976 -0.828845  \n",
       "breathes  0.369868  0.088952  0.124068  0.204125 -0.519308  \n",
       "venomous  0.022964 -0.162724 -0.003252 -0.183748  0.321476  \n",
       "fins     -0.605652  0.204349 -0.093887  0.031705  0.099430  \n",
       "legs      1.000000 -0.348295  0.073931  0.068791  0.131693  \n",
       "tail     -0.348295  1.000000  0.023434  0.243277 -0.631830  \n",
       "domestic  0.073931  0.023434  1.000000  0.020073 -0.181043  \n",
       "catsize   0.068791  0.243277  0.020073  1.000000 -0.521030  \n",
       "type      0.131693 -0.631830 -0.181043 -0.521030  1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b3c4c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of     animal name  hair  feathers  eggs  milk  airborne  aquatic  predator  \\\n",
       "0      aardvark     1         0     0     1         0        0         1   \n",
       "1      antelope     1         0     0     1         0        0         0   \n",
       "2          bass     0         0     1     0         0        1         1   \n",
       "3          bear     1         0     0     1         0        0         1   \n",
       "4          boar     1         0     0     1         0        0         1   \n",
       "..          ...   ...       ...   ...   ...       ...      ...       ...   \n",
       "96      wallaby     1         0     0     1         0        0         0   \n",
       "97         wasp     1         0     1     0         1        0         0   \n",
       "98         wolf     1         0     0     1         0        0         1   \n",
       "99         worm     0         0     1     0         0        0         0   \n",
       "100        wren     0         1     1     0         1        0         0   \n",
       "\n",
       "     toothed  backbone  breathes  venomous  fins  legs  tail  domestic  \\\n",
       "0          1         1         1         0     0     4     0         0   \n",
       "1          1         1         1         0     0     4     1         0   \n",
       "2          1         1         0         0     1     0     1         0   \n",
       "3          1         1         1         0     0     4     0         0   \n",
       "4          1         1         1         0     0     4     1         0   \n",
       "..       ...       ...       ...       ...   ...   ...   ...       ...   \n",
       "96         1         1         1         0     0     2     1         0   \n",
       "97         0         0         1         1     0     6     0         0   \n",
       "98         1         1         1         0     0     4     1         0   \n",
       "99         0         0         1         0     0     0     0         0   \n",
       "100        0         1         1         0     0     2     1         0   \n",
       "\n",
       "     catsize  type  \n",
       "0          1     1  \n",
       "1          1     1  \n",
       "2          0     4  \n",
       "3          1     1  \n",
       "4          1     1  \n",
       "..       ...   ...  \n",
       "96         1     1  \n",
       "97         0     6  \n",
       "98         1     1  \n",
       "99         0     7  \n",
       "100        0     2  \n",
       "\n",
       "[101 rows x 18 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed03544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc3740a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.boxplot(vert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ea65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the counts of unique values in the 'type' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c5b7246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    41\n",
       "2    20\n",
       "4    13\n",
       "7    10\n",
       "6     8\n",
       "3     5\n",
       "5     4\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a9e6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LabelEncoder used to convert categorical labels into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23105956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal name</th>\n",
       "      <th>hair</th>\n",
       "      <th>feathers</th>\n",
       "      <th>eggs</th>\n",
       "      <th>milk</th>\n",
       "      <th>airborne</th>\n",
       "      <th>aquatic</th>\n",
       "      <th>predator</th>\n",
       "      <th>toothed</th>\n",
       "      <th>backbone</th>\n",
       "      <th>breathes</th>\n",
       "      <th>venomous</th>\n",
       "      <th>fins</th>\n",
       "      <th>legs</th>\n",
       "      <th>tail</th>\n",
       "      <th>domestic</th>\n",
       "      <th>catsize</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     animal name  hair  feathers  eggs  milk  airborne  aquatic  predator  \\\n",
       "0              0     1         0     0     1         0        0         1   \n",
       "1              1     1         0     0     1         0        0         0   \n",
       "2              2     0         0     1     0         0        1         1   \n",
       "3              3     1         0     0     1         0        0         1   \n",
       "4              4     1         0     0     1         0        0         1   \n",
       "..           ...   ...       ...   ...   ...       ...      ...       ...   \n",
       "96            95     1         0     0     1         0        0         0   \n",
       "97            96     1         0     1     0         1        0         0   \n",
       "98            97     1         0     0     1         0        0         1   \n",
       "99            98     0         0     1     0         0        0         0   \n",
       "100           99     0         1     1     0         1        0         0   \n",
       "\n",
       "     toothed  backbone  breathes  venomous  fins  legs  tail  domestic  \\\n",
       "0          1         1         1         0     0     4     0         0   \n",
       "1          1         1         1         0     0     4     1         0   \n",
       "2          1         1         0         0     1     0     1         0   \n",
       "3          1         1         1         0     0     4     0         0   \n",
       "4          1         1         1         0     0     4     1         0   \n",
       "..       ...       ...       ...       ...   ...   ...   ...       ...   \n",
       "96         1         1         1         0     0     2     1         0   \n",
       "97         0         0         1         1     0     6     0         0   \n",
       "98         1         1         1         0     0     4     1         0   \n",
       "99         0         0         1         0     0     0     0         0   \n",
       "100        0         1         1         0     0     2     1         0   \n",
       "\n",
       "     catsize  type  \n",
       "0          1     1  \n",
       "1          1     1  \n",
       "2          0     4  \n",
       "3          1     1  \n",
       "4          1     1  \n",
       "..       ...   ...  \n",
       "96         1     1  \n",
       "97         0     6  \n",
       "98         1     1  \n",
       "99         0     7  \n",
       "100        0     2  \n",
       "\n",
       "[101 rows x 18 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE = LabelEncoder()\n",
    "df['animal name'] = LE.fit_transform(df['animal name'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d42e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting your DataFrame df into features (X) and the target variable (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef51b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:17]\n",
    "#(or)X = df.drop('type',axis=1)\n",
    "Y = df['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1049070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split your data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "144d7aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70, 17), (31, 17), (70,), (31,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3)\n",
    "X_train.shape,X_test.shape,Y_train.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd8a1260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applied standardization to your feature matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d50fe7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal name</th>\n",
       "      <th>hair</th>\n",
       "      <th>feathers</th>\n",
       "      <th>eggs</th>\n",
       "      <th>milk</th>\n",
       "      <th>airborne</th>\n",
       "      <th>aquatic</th>\n",
       "      <th>predator</th>\n",
       "      <th>toothed</th>\n",
       "      <th>backbone</th>\n",
       "      <th>breathes</th>\n",
       "      <th>venomous</th>\n",
       "      <th>fins</th>\n",
       "      <th>legs</th>\n",
       "      <th>tail</th>\n",
       "      <th>domestic</th>\n",
       "      <th>catsize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.708840</td>\n",
       "      <td>1.161395</td>\n",
       "      <td>-0.496904</td>\n",
       "      <td>-1.185227</td>\n",
       "      <td>1.209717</td>\n",
       "      <td>-0.558291</td>\n",
       "      <td>-0.744208</td>\n",
       "      <td>0.896421</td>\n",
       "      <td>0.809776</td>\n",
       "      <td>0.46569</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>-0.293294</td>\n",
       "      <td>-0.449868</td>\n",
       "      <td>0.572540</td>\n",
       "      <td>-1.698416</td>\n",
       "      <td>-0.384353</td>\n",
       "      <td>1.138180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.674148</td>\n",
       "      <td>1.161395</td>\n",
       "      <td>-0.496904</td>\n",
       "      <td>-1.185227</td>\n",
       "      <td>1.209717</td>\n",
       "      <td>-0.558291</td>\n",
       "      <td>-0.744208</td>\n",
       "      <td>-1.115547</td>\n",
       "      <td>0.809776</td>\n",
       "      <td>0.46569</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>-0.293294</td>\n",
       "      <td>-0.449868</td>\n",
       "      <td>0.572540</td>\n",
       "      <td>0.588784</td>\n",
       "      <td>-0.384353</td>\n",
       "      <td>1.138180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.639456</td>\n",
       "      <td>-0.861034</td>\n",
       "      <td>-0.496904</td>\n",
       "      <td>0.843721</td>\n",
       "      <td>-0.826640</td>\n",
       "      <td>-0.558291</td>\n",
       "      <td>1.343710</td>\n",
       "      <td>0.896421</td>\n",
       "      <td>0.809776</td>\n",
       "      <td>0.46569</td>\n",
       "      <td>-1.951800</td>\n",
       "      <td>-0.293294</td>\n",
       "      <td>2.222876</td>\n",
       "      <td>-1.404435</td>\n",
       "      <td>0.588784</td>\n",
       "      <td>-0.384353</td>\n",
       "      <td>-0.878595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.604764</td>\n",
       "      <td>1.161395</td>\n",
       "      <td>-0.496904</td>\n",
       "      <td>-1.185227</td>\n",
       "      <td>1.209717</td>\n",
       "      <td>-0.558291</td>\n",
       "      <td>-0.744208</td>\n",
       "      <td>0.896421</td>\n",
       "      <td>0.809776</td>\n",
       "      <td>0.46569</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>-0.293294</td>\n",
       "      <td>-0.449868</td>\n",
       "      <td>0.572540</td>\n",
       "      <td>-1.698416</td>\n",
       "      <td>-0.384353</td>\n",
       "      <td>1.138180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.570072</td>\n",
       "      <td>1.161395</td>\n",
       "      <td>-0.496904</td>\n",
       "      <td>-1.185227</td>\n",
       "      <td>1.209717</td>\n",
       "      <td>-0.558291</td>\n",
       "      <td>-0.744208</td>\n",
       "      <td>0.896421</td>\n",
       "      <td>0.809776</td>\n",
       "      <td>0.46569</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>-0.293294</td>\n",
       "      <td>-0.449868</td>\n",
       "      <td>0.572540</td>\n",
       "      <td>0.588784</td>\n",
       "      <td>-0.384353</td>\n",
       "      <td>1.138180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.586903</td>\n",
       "      <td>1.161395</td>\n",
       "      <td>-0.496904</td>\n",
       "      <td>-1.185227</td>\n",
       "      <td>1.209717</td>\n",
       "      <td>-0.558291</td>\n",
       "      <td>-0.744208</td>\n",
       "      <td>-1.115547</td>\n",
       "      <td>0.809776</td>\n",
       "      <td>0.46569</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>-0.293294</td>\n",
       "      <td>-0.449868</td>\n",
       "      <td>-0.415948</td>\n",
       "      <td>0.588784</td>\n",
       "      <td>-0.384353</td>\n",
       "      <td>1.138180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.621595</td>\n",
       "      <td>1.161395</td>\n",
       "      <td>-0.496904</td>\n",
       "      <td>0.843721</td>\n",
       "      <td>-0.826640</td>\n",
       "      <td>1.791182</td>\n",
       "      <td>-0.744208</td>\n",
       "      <td>-1.115547</td>\n",
       "      <td>-1.234909</td>\n",
       "      <td>-2.14735</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>3.409545</td>\n",
       "      <td>-0.449868</td>\n",
       "      <td>1.561027</td>\n",
       "      <td>-1.698416</td>\n",
       "      <td>-0.384353</td>\n",
       "      <td>-0.878595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.656287</td>\n",
       "      <td>1.161395</td>\n",
       "      <td>-0.496904</td>\n",
       "      <td>-1.185227</td>\n",
       "      <td>1.209717</td>\n",
       "      <td>-0.558291</td>\n",
       "      <td>-0.744208</td>\n",
       "      <td>0.896421</td>\n",
       "      <td>0.809776</td>\n",
       "      <td>0.46569</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>-0.293294</td>\n",
       "      <td>-0.449868</td>\n",
       "      <td>0.572540</td>\n",
       "      <td>0.588784</td>\n",
       "      <td>-0.384353</td>\n",
       "      <td>1.138180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.690979</td>\n",
       "      <td>-0.861034</td>\n",
       "      <td>-0.496904</td>\n",
       "      <td>0.843721</td>\n",
       "      <td>-0.826640</td>\n",
       "      <td>-0.558291</td>\n",
       "      <td>-0.744208</td>\n",
       "      <td>-1.115547</td>\n",
       "      <td>-1.234909</td>\n",
       "      <td>-2.14735</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>-0.293294</td>\n",
       "      <td>-0.449868</td>\n",
       "      <td>-1.404435</td>\n",
       "      <td>-1.698416</td>\n",
       "      <td>-0.384353</td>\n",
       "      <td>-0.878595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.725671</td>\n",
       "      <td>-0.861034</td>\n",
       "      <td>2.012461</td>\n",
       "      <td>0.843721</td>\n",
       "      <td>-0.826640</td>\n",
       "      <td>1.791182</td>\n",
       "      <td>-0.744208</td>\n",
       "      <td>-1.115547</td>\n",
       "      <td>-1.234909</td>\n",
       "      <td>0.46569</td>\n",
       "      <td>0.512348</td>\n",
       "      <td>-0.293294</td>\n",
       "      <td>-0.449868</td>\n",
       "      <td>-0.415948</td>\n",
       "      <td>0.588784</td>\n",
       "      <td>-0.384353</td>\n",
       "      <td>-0.878595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     animal name      hair  feathers      eggs      milk  airborne   aquatic  \\\n",
       "0      -1.708840  1.161395 -0.496904 -1.185227  1.209717 -0.558291 -0.744208   \n",
       "1      -1.674148  1.161395 -0.496904 -1.185227  1.209717 -0.558291 -0.744208   \n",
       "2      -1.639456 -0.861034 -0.496904  0.843721 -0.826640 -0.558291  1.343710   \n",
       "3      -1.604764  1.161395 -0.496904 -1.185227  1.209717 -0.558291 -0.744208   \n",
       "4      -1.570072  1.161395 -0.496904 -1.185227  1.209717 -0.558291 -0.744208   \n",
       "..           ...       ...       ...       ...       ...       ...       ...   \n",
       "96      1.586903  1.161395 -0.496904 -1.185227  1.209717 -0.558291 -0.744208   \n",
       "97      1.621595  1.161395 -0.496904  0.843721 -0.826640  1.791182 -0.744208   \n",
       "98      1.656287  1.161395 -0.496904 -1.185227  1.209717 -0.558291 -0.744208   \n",
       "99      1.690979 -0.861034 -0.496904  0.843721 -0.826640 -0.558291 -0.744208   \n",
       "100     1.725671 -0.861034  2.012461  0.843721 -0.826640  1.791182 -0.744208   \n",
       "\n",
       "     predator   toothed  backbone  breathes  venomous      fins      legs  \\\n",
       "0    0.896421  0.809776   0.46569  0.512348 -0.293294 -0.449868  0.572540   \n",
       "1   -1.115547  0.809776   0.46569  0.512348 -0.293294 -0.449868  0.572540   \n",
       "2    0.896421  0.809776   0.46569 -1.951800 -0.293294  2.222876 -1.404435   \n",
       "3    0.896421  0.809776   0.46569  0.512348 -0.293294 -0.449868  0.572540   \n",
       "4    0.896421  0.809776   0.46569  0.512348 -0.293294 -0.449868  0.572540   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "96  -1.115547  0.809776   0.46569  0.512348 -0.293294 -0.449868 -0.415948   \n",
       "97  -1.115547 -1.234909  -2.14735  0.512348  3.409545 -0.449868  1.561027   \n",
       "98   0.896421  0.809776   0.46569  0.512348 -0.293294 -0.449868  0.572540   \n",
       "99  -1.115547 -1.234909  -2.14735  0.512348 -0.293294 -0.449868 -1.404435   \n",
       "100 -1.115547 -1.234909   0.46569  0.512348 -0.293294 -0.449868 -0.415948   \n",
       "\n",
       "         tail  domestic   catsize  \n",
       "0   -1.698416 -0.384353  1.138180  \n",
       "1    0.588784 -0.384353  1.138180  \n",
       "2    0.588784 -0.384353 -0.878595  \n",
       "3   -1.698416 -0.384353  1.138180  \n",
       "4    0.588784 -0.384353  1.138180  \n",
       "..        ...       ...       ...  \n",
       "96   0.588784 -0.384353  1.138180  \n",
       "97  -1.698416 -0.384353 -0.878595  \n",
       "98   0.588784 -0.384353  1.138180  \n",
       "99  -1.698416 -0.384353 -0.878595  \n",
       "100  0.588784 -0.384353 -0.878595  \n",
       "\n",
       "[101 rows x 17 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "ss_x = ss.fit_transform(X)\n",
    "ss_x = pd.DataFrame(ss_x)\n",
    "ss_x.columns = list(X)\n",
    "ss_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "879035cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a KNeighborsClassifier instance with n_neighbors=5 and p=2 (Euclidean distance),then fit the model to your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24a1399b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5,p=2)#p-->2 for eucleidian distance\n",
    "knn.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00ab7b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain predictions on both the training set (X_train) and the test set (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4afd326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train = knn.predict(X_train)\n",
    "Y_pred_test = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd284997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the performance of your KNN model on both the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd2e1441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.5428571428571428\n",
      "Test score is 0.3870967741935484\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62aba80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the performance of a k-nearest neighbors (knn) classifier. The training and test accuracy scores are computed for each fold, and then the average scores are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4097fb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.9678086419753086\n",
      "Test score is 0.8899999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df):\n",
    "    x_train, x_test = ss_x.iloc[train_index], ss_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_pred_train = knn.predict(x_train)\n",
    "    y_pred_test = knn.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3531b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on the training data, makes predictions on both the training and test sets, and then prints the training and test accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b24a435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7428571428571429\n",
      "Test score is 0.6129032258064516\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5,p=1)#p-->1 for manhattan distance\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred_train = knn.predict(X_train)\n",
    "Y_pred_test = knn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a86b61da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross-validation for evaluating the performance of the k-nearest neighbors classifier. This allows a more robust assessment by splitting the data into multiple folds and calculating average scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e3af705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.9628395061728394\n",
      "Test score is 0.9099999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df):\n",
    "    x_train, x_test = ss_x.iloc[train_index], ss_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_pred_train = knn.predict(x_train)\n",
    "    y_pred_test = knn.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "691c89c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on the training data, makes predictions on both the training and test sets, and then prints the training and test accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11acc3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.5571428571428572\n",
      "Test score is 0.41935483870967744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=7,p=2)\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred_train = knn.predict(X_train)\n",
    "Y_pred_test = knn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6b7c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross-validation for evaluating the performance of the k-nearest neighbors classifier. This allows a more robust assessment by splitting the data into multiple folds and calculating average scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d2a2e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.9281481481481482\n",
      "Test score is 0.840952380952381\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df):\n",
    "    x_train, x_test = ss_x.iloc[train_index], ss_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_pred_train = knn.predict(x_train)\n",
    "    y_pred_test = knn.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f77a6ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on the training data, makes predictions on both the training and test sets, and then prints the training and test accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1750b216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.6428571428571429\n",
      "Test score is 0.5806451612903226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=7,p=1)\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred_train = knn.predict(X_train)\n",
    "Y_pred_test = knn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49ff288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross-validation for evaluating the performance of the k-nearest neighbors classifier. This allows a more robust assessment by splitting the data into multiple folds and calculating average scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80bcfef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.9455555555555556\n",
      "Test score is 0.89\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df):\n",
    "    x_train, x_test = ss_x.iloc[train_index], ss_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_pred_train = knn.predict(x_train)\n",
    "    y_pred_test = knn.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "706ecd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on the training data, makes predictions on both the training and test sets, and then prints the training and test accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d054c1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.42857142857142855\n",
      "Test score is 0.3548387096774194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=10,p=2)\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred_train = knn.predict(X_train)\n",
    "Y_pred_test = knn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c197a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross-validation for evaluating the performance of the k-nearest neighbors classifier. This allows a more robust assessment by splitting the data into multiple folds and calculating average scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65925f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.9058641975308642\n",
      "Test score is 0.8304761904761904\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df):\n",
    "    x_train, x_test = ss_x.iloc[train_index], ss_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_pred_train = knn.predict(x_train)\n",
    "    y_pred_test = knn.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "837d5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on the training data, makes predictions on both the training and test sets, and then prints the training and test accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a99755bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.6\n",
      "Test score is 0.4838709677419355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=10,p=1)\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred_train = knn.predict(X_train)\n",
    "Y_pred_test = knn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c150693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross-validation for evaluating the performance of the k-nearest neighbors classifier. This allows a more robust assessment by splitting the data into multiple folds and calculating average scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b12ae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.9256790123456791\n",
      "Test score is 0.840952380952381\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df):\n",
    "    x_train, x_test = ss_x.iloc[train_index], ss_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_pred_train = knn.predict(x_train)\n",
    "    y_pred_test = knn.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d513feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on the training data, makes predictions on both the training and test sets, and then prints the training and test accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "346fdc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.6142857142857143\n",
      "Test score is 0.41935483870967744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=4,p=2)\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred_train = knn.predict(X_train)\n",
    "Y_pred_test = knn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "802cf471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross-validation for evaluating the performance of the k-nearest neighbors classifier. This allows a more robust assessment by splitting the data into multiple folds and calculating average scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1427e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.9603395061728394\n",
      "Test score is 0.9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df):\n",
    "    x_train, x_test = ss_x.iloc[train_index], ss_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_pred_train = knn.predict(x_train)\n",
    "    y_pred_test = knn.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d30014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on the training data, makes predictions on both the training and test sets, and then prints the training and test accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7365e100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7857142857142857\n",
      "Test score is 0.6774193548387096\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=4,p=1)\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred_train = knn.predict(X_train)\n",
    "Y_pred_test = knn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21ed6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross-validation for evaluating the performance of the k-nearest neighbors classifier. This allows a more robust assessment by splitting the data into multiple folds and calculating average scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79a196fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.9652777777777777\n",
      "Test score is 0.9099999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df):\n",
    "    x_train, x_test = ss_x.iloc[train_index], ss_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_pred_train = knn.predict(x_train)\n",
    "    y_pred_test = knn.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "303b2ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on the training data, makes predictions on both the training and test sets, and then prints the training and test accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6f4abb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7142857142857143\n",
      "Test score is 0.4838709677419355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=2,p=2)\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred_train = knn.predict(X_train)\n",
    "Y_pred_test = knn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0cc63f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross-validation for evaluating the performance of the k-nearest neighbors classifier. This allows a more robust assessment by splitting the data into multiple folds and calculating average scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "53e1273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.975246913580247\n",
      "Test score is 0.9199999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df):\n",
    "    x_train, x_test = ss_x.iloc[train_index], ss_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_pred_train = knn.predict(x_train)\n",
    "    y_pred_test = knn.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0326e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on the training data, makes predictions on both the training and test sets, and then prints the training and test accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8052ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.8571428571428571\n",
      "Test score is 0.8387096774193549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=2,p=1)\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred_train = knn.predict(X_train)\n",
    "Y_pred_test = knn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6b9904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross-validation for evaluating the performance of the k-nearest neighbors classifier. This allows a more robust assessment by splitting the data into multiple folds and calculating average scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4e4ecaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.980185185185185\n",
      "Test score is 0.9299999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df):\n",
    "    x_train, x_test = ss_x.iloc[train_index], ss_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_pred_train = knn.predict(x_train)\n",
    "    y_pred_test = knn.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eaed595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on the training data, makes predictions on both the training and test sets, and then prints the training and test accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d7edd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 1.0\n",
      "Test score is 0.5161290322580645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1,p=2)\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred_train = knn.predict(X_train)\n",
    "Y_pred_test = knn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40c40c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross-validation for evaluating the performance of the k-nearest neighbors classifier. This allows a more robust assessment by splitting the data into multiple folds and calculating average scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b3590d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 1.0\n",
      "Test score is 0.93\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df):\n",
    "    x_train, x_test = ss_x.iloc[train_index], ss_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_pred_train = knn.predict(x_train)\n",
    "    y_pred_test = knn.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a47ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on the training data, makes predictions on both the training and test sets, and then prints the training and test accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9076d700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 1.0\n",
      "Test score is 0.7419354838709677\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1,p=1)\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred_train = knn.predict(X_train)\n",
    "Y_pred_test = knn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ea24c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross-validation for evaluating the performance of the k-nearest neighbors classifier. This allows a more robust assessment by splitting the data into multiple folds and calculating average scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "616ce243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 1.0\n",
      "Test score is 0.95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df):\n",
    "    x_train, x_test = ss_x.iloc[train_index], ss_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_pred_train = knn.predict(x_train)\n",
    "    y_pred_test = knn.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38327c49",
   "metadata": {},
   "source": [
    "# **As we see from the above data we conclude that by increasing n_neighbors the Accuracy Score will decrease and by using\n",
    "# **Manhattan distance we are getting high Accuracy Score than Eucledian distance and by decreasing n_neighbors the Accuracy Score \n",
    "# will get increasing.\n",
    "# **And also by using kfold we are getting a better improvement in every knn model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084446d5",
   "metadata": {},
   "source": [
    "# Q2).Prepare a model for glass classification using KNN\n",
    "\n",
    "# Data Description:\n",
    "\n",
    "# RI : refractive index\n",
    "\n",
    "# Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n",
    "\n",
    "# Mg: Magnesium\n",
    "\n",
    "# AI: Aluminum\n",
    "\n",
    "# Si: Silicon\n",
    "\n",
    "# K:Potassium\n",
    "\n",
    "# Ca: Calcium\n",
    "\n",
    "# Ba: Barium\n",
    "\n",
    "# Fe: Iron\n",
    "\n",
    "# Type: Type of glass: (class attribute)\n",
    "# 1 -- building_windows_float_processed\n",
    "# 2 --building_windows_non_float_processed\n",
    "# 3 --vehicle_windows_float_processed\n",
    "# 4 --vehicle_windows_non_float_processed (none in this database)\n",
    "# 5 --containers\n",
    "# 6 --tableware\n",
    "# 7 --headlamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23411177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pandas and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0424369f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.51761</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.51618</td>\n",
       "      <td>13.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RI     Na    Mg    Al     Si     K    Ca   Ba   Fe  Type\n",
       "0  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0     1\n",
       "1  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0     1\n",
       "2  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0     1\n",
       "3  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0     1\n",
       "4  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0     1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_csv('glass.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f6dfe939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "79e43ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.518365</td>\n",
       "      <td>13.407850</td>\n",
       "      <td>2.684533</td>\n",
       "      <td>1.444907</td>\n",
       "      <td>72.650935</td>\n",
       "      <td>0.497056</td>\n",
       "      <td>8.956963</td>\n",
       "      <td>0.175047</td>\n",
       "      <td>0.057009</td>\n",
       "      <td>2.780374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.816604</td>\n",
       "      <td>1.442408</td>\n",
       "      <td>0.499270</td>\n",
       "      <td>0.774546</td>\n",
       "      <td>0.652192</td>\n",
       "      <td>1.423153</td>\n",
       "      <td>0.497219</td>\n",
       "      <td>0.097439</td>\n",
       "      <td>2.103739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.511150</td>\n",
       "      <td>10.730000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>69.810000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.430000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.516522</td>\n",
       "      <td>12.907500</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>8.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.517680</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>72.790000</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.519157</td>\n",
       "      <td>13.825000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.630000</td>\n",
       "      <td>73.087500</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>9.172500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.533930</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>4.490000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>75.410000</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>16.190000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               RI          Na          Mg          Al          Si           K  \\\n",
       "count  214.000000  214.000000  214.000000  214.000000  214.000000  214.000000   \n",
       "mean     1.518365   13.407850    2.684533    1.444907   72.650935    0.497056   \n",
       "std      0.003037    0.816604    1.442408    0.499270    0.774546    0.652192   \n",
       "min      1.511150   10.730000    0.000000    0.290000   69.810000    0.000000   \n",
       "25%      1.516522   12.907500    2.115000    1.190000   72.280000    0.122500   \n",
       "50%      1.517680   13.300000    3.480000    1.360000   72.790000    0.555000   \n",
       "75%      1.519157   13.825000    3.600000    1.630000   73.087500    0.610000   \n",
       "max      1.533930   17.380000    4.490000    3.500000   75.410000    6.210000   \n",
       "\n",
       "               Ca          Ba          Fe        Type  \n",
       "count  214.000000  214.000000  214.000000  214.000000  \n",
       "mean     8.956963    0.175047    0.057009    2.780374  \n",
       "std      1.423153    0.497219    0.097439    2.103739  \n",
       "min      5.430000    0.000000    0.000000    1.000000  \n",
       "25%      8.240000    0.000000    0.000000    1.000000  \n",
       "50%      8.600000    0.000000    0.000000    2.000000  \n",
       "75%      9.172500    0.000000    0.100000    3.000000  \n",
       "max     16.190000    3.150000    0.510000    7.000000  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ccac1e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RI</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.191885</td>\n",
       "      <td>-0.122274</td>\n",
       "      <td>-0.407326</td>\n",
       "      <td>-0.542052</td>\n",
       "      <td>-0.289833</td>\n",
       "      <td>0.810403</td>\n",
       "      <td>-0.000386</td>\n",
       "      <td>0.143010</td>\n",
       "      <td>-0.164237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Na</th>\n",
       "      <td>-0.191885</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.273732</td>\n",
       "      <td>0.156794</td>\n",
       "      <td>-0.069809</td>\n",
       "      <td>-0.266087</td>\n",
       "      <td>-0.275442</td>\n",
       "      <td>0.326603</td>\n",
       "      <td>-0.241346</td>\n",
       "      <td>0.502898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mg</th>\n",
       "      <td>-0.122274</td>\n",
       "      <td>-0.273732</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.481799</td>\n",
       "      <td>-0.165927</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>-0.443750</td>\n",
       "      <td>-0.492262</td>\n",
       "      <td>0.083060</td>\n",
       "      <td>-0.744993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Al</th>\n",
       "      <td>-0.407326</td>\n",
       "      <td>0.156794</td>\n",
       "      <td>-0.481799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005524</td>\n",
       "      <td>0.325958</td>\n",
       "      <td>-0.259592</td>\n",
       "      <td>0.479404</td>\n",
       "      <td>-0.074402</td>\n",
       "      <td>0.598829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Si</th>\n",
       "      <td>-0.542052</td>\n",
       "      <td>-0.069809</td>\n",
       "      <td>-0.165927</td>\n",
       "      <td>-0.005524</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.193331</td>\n",
       "      <td>-0.208732</td>\n",
       "      <td>-0.102151</td>\n",
       "      <td>-0.094201</td>\n",
       "      <td>0.151565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>-0.289833</td>\n",
       "      <td>-0.266087</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.325958</td>\n",
       "      <td>-0.193331</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.317836</td>\n",
       "      <td>-0.042618</td>\n",
       "      <td>-0.007719</td>\n",
       "      <td>-0.010054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ca</th>\n",
       "      <td>0.810403</td>\n",
       "      <td>-0.275442</td>\n",
       "      <td>-0.443750</td>\n",
       "      <td>-0.259592</td>\n",
       "      <td>-0.208732</td>\n",
       "      <td>-0.317836</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.112841</td>\n",
       "      <td>0.124968</td>\n",
       "      <td>0.000952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ba</th>\n",
       "      <td>-0.000386</td>\n",
       "      <td>0.326603</td>\n",
       "      <td>-0.492262</td>\n",
       "      <td>0.479404</td>\n",
       "      <td>-0.102151</td>\n",
       "      <td>-0.042618</td>\n",
       "      <td>-0.112841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.058692</td>\n",
       "      <td>0.575161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fe</th>\n",
       "      <td>0.143010</td>\n",
       "      <td>-0.241346</td>\n",
       "      <td>0.083060</td>\n",
       "      <td>-0.074402</td>\n",
       "      <td>-0.094201</td>\n",
       "      <td>-0.007719</td>\n",
       "      <td>0.124968</td>\n",
       "      <td>-0.058692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.188278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>-0.164237</td>\n",
       "      <td>0.502898</td>\n",
       "      <td>-0.744993</td>\n",
       "      <td>0.598829</td>\n",
       "      <td>0.151565</td>\n",
       "      <td>-0.010054</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.575161</td>\n",
       "      <td>-0.188278</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            RI        Na        Mg        Al        Si         K        Ca  \\\n",
       "RI    1.000000 -0.191885 -0.122274 -0.407326 -0.542052 -0.289833  0.810403   \n",
       "Na   -0.191885  1.000000 -0.273732  0.156794 -0.069809 -0.266087 -0.275442   \n",
       "Mg   -0.122274 -0.273732  1.000000 -0.481799 -0.165927  0.005396 -0.443750   \n",
       "Al   -0.407326  0.156794 -0.481799  1.000000 -0.005524  0.325958 -0.259592   \n",
       "Si   -0.542052 -0.069809 -0.165927 -0.005524  1.000000 -0.193331 -0.208732   \n",
       "K    -0.289833 -0.266087  0.005396  0.325958 -0.193331  1.000000 -0.317836   \n",
       "Ca    0.810403 -0.275442 -0.443750 -0.259592 -0.208732 -0.317836  1.000000   \n",
       "Ba   -0.000386  0.326603 -0.492262  0.479404 -0.102151 -0.042618 -0.112841   \n",
       "Fe    0.143010 -0.241346  0.083060 -0.074402 -0.094201 -0.007719  0.124968   \n",
       "Type -0.164237  0.502898 -0.744993  0.598829  0.151565 -0.010054  0.000952   \n",
       "\n",
       "            Ba        Fe      Type  \n",
       "RI   -0.000386  0.143010 -0.164237  \n",
       "Na    0.326603 -0.241346  0.502898  \n",
       "Mg   -0.492262  0.083060 -0.744993  \n",
       "Al    0.479404 -0.074402  0.598829  \n",
       "Si   -0.102151 -0.094201  0.151565  \n",
       "K    -0.042618 -0.007719 -0.010054  \n",
       "Ca   -0.112841  0.124968  0.000952  \n",
       "Ba    1.000000 -0.058692  0.575161  \n",
       "Fe   -0.058692  1.000000 -0.188278  \n",
       "Type  0.575161 -0.188278  1.000000  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "589fb101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 214 entries, 0 to 213\n",
      "Data columns (total 10 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   RI      214 non-null    float64\n",
      " 1   Na      214 non-null    float64\n",
      " 2   Mg      214 non-null    float64\n",
      " 3   Al      214 non-null    float64\n",
      " 4   Si      214 non-null    float64\n",
      " 5   K       214 non-null    float64\n",
      " 6   Ca      214 non-null    float64\n",
      " 7   Ba      214 non-null    float64\n",
      " 8   Fe      214 non-null    float64\n",
      " 9   Type    214 non-null    int64  \n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 16.8 KB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "01f25b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the counts of unique values in the 'type' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9619f38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    76\n",
       "1    70\n",
       "7    29\n",
       "3    17\n",
       "5    13\n",
       "6     9\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6dd600f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting your DataFrame df into features (X) and the target variable (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3879954",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.iloc[:,0:9]\n",
    "Y = df2['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bca2d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applied standardization to your feature matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b07344a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.432836</td>\n",
       "      <td>0.437594</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.351786</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.308550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.283582</td>\n",
       "      <td>0.475188</td>\n",
       "      <td>0.801782</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.521429</td>\n",
       "      <td>0.077295</td>\n",
       "      <td>0.223048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.220808</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.790646</td>\n",
       "      <td>0.389408</td>\n",
       "      <td>0.567857</td>\n",
       "      <td>0.062802</td>\n",
       "      <td>0.218401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.285777</td>\n",
       "      <td>0.372932</td>\n",
       "      <td>0.821826</td>\n",
       "      <td>0.311526</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.091787</td>\n",
       "      <td>0.259294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.275241</td>\n",
       "      <td>0.381955</td>\n",
       "      <td>0.806236</td>\n",
       "      <td>0.295950</td>\n",
       "      <td>0.583929</td>\n",
       "      <td>0.088567</td>\n",
       "      <td>0.245353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.223003</td>\n",
       "      <td>0.512782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.806854</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.012882</td>\n",
       "      <td>0.348513</td>\n",
       "      <td>0.336508</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.250219</td>\n",
       "      <td>0.630075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.529595</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276022</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.417032</td>\n",
       "      <td>0.545865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538941</td>\n",
       "      <td>0.644643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279740</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.548872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283457</td>\n",
       "      <td>0.498413</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.261633</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.557632</td>\n",
       "      <td>0.633929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296468</td>\n",
       "      <td>0.530159</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           RI        Na        Mg        Al        Si         K        Ca  \\\n",
       "0    0.432836  0.437594  1.000000  0.252336  0.351786  0.009662  0.308550   \n",
       "1    0.283582  0.475188  0.801782  0.333333  0.521429  0.077295  0.223048   \n",
       "2    0.220808  0.421053  0.790646  0.389408  0.567857  0.062802  0.218401   \n",
       "3    0.285777  0.372932  0.821826  0.311526  0.500000  0.091787  0.259294   \n",
       "4    0.275241  0.381955  0.806236  0.295950  0.583929  0.088567  0.245353   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "209  0.223003  0.512782  0.000000  0.806854  0.500000  0.012882  0.348513   \n",
       "210  0.250219  0.630075  0.000000  0.529595  0.580357  0.000000  0.276022   \n",
       "211  0.417032  0.545865  0.000000  0.538941  0.644643  0.000000  0.279740   \n",
       "212  0.235294  0.548872  0.000000  0.514019  0.678571  0.000000  0.283457   \n",
       "213  0.261633  0.526316  0.000000  0.557632  0.633929  0.000000  0.296468   \n",
       "\n",
       "           Ba   Fe  \n",
       "0    0.000000  0.0  \n",
       "1    0.000000  0.0  \n",
       "2    0.000000  0.0  \n",
       "3    0.000000  0.0  \n",
       "4    0.000000  0.0  \n",
       "..        ...  ...  \n",
       "209  0.336508  0.0  \n",
       "210  0.504762  0.0  \n",
       "211  0.520635  0.0  \n",
       "212  0.498413  0.0  \n",
       "213  0.530159  0.0  \n",
       "\n",
       "[214 rows x 9 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "mm_x = mm.fit_transform(X)\n",
    "mm_x = pd.DataFrame(mm_x)\n",
    "mm_x.columns = list(X)\n",
    "mm_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "03f327cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split your data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e5ecd5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149, 9), (65, 9), (149,), (65,))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3)\n",
    "X_train.shape,X_test.shape,Y_train.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7e84d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the KNeighborsClassifier and  fits the KNN classifier on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4317618b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=5,p=2)\n",
    "knc.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5f273d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions on both the training set (X_train) and the test set (X_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6239deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "03e27e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the performance of your KNN model on both the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6a0cdff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7516778523489933\n",
      "Test score is 0.5846153846153846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7098da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c7ca0bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7873997008023935\n",
      "Test score is 0.3165005537098561\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ab9b0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained the model on a training set and evaluated its performance on both the training and test sets,calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "653a589b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.785234899328859\n",
      "Test score is 0.6615384615384615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=5,p=1)\n",
    "knc.fit(X_train,Y_train)\n",
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "71a0d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8c184655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7978988168094656\n",
      "Test score is 0.3304540420819491\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b9ae972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained the model on a training set and evaluated its performance on both the training and test sets,calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eb6d1937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.738255033557047\n",
      "Test score is 0.5692307692307692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=6,p=2)\n",
    "knc.fit(X_train,Y_train)\n",
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a7498734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2fb4465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7570039439684482\n",
      "Test score is 0.27918050941306755\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d3d1db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained the model on a training set and evaluated its performance on both the training and test sets,calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b66d1e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7516778523489933\n",
      "Test score is 0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=6,p=1)\n",
    "knc.fit(X_train,Y_train)\n",
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a367bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "775044c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7920848633210935\n",
      "Test score is 0.3117386489479513\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "668d86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained the model on a training set and evaluated its performance on both the training and test sets,calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f2b66048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7181208053691275\n",
      "Test score is 0.5538461538461539\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=8,p=2)\n",
    "knc.fit(X_train,Y_train)\n",
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "41ec962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d1d02bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7429892560859513\n",
      "Test score is 0.2791805094130676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2ec2e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained the model on a training set and evaluated its performance on both the training and test sets,calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "84df804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7046979865771812\n",
      "Test score is 0.5692307692307692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=8,p=1)\n",
    "knc.fit(X_train,Y_train)\n",
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9baff13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fc69986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7558479532163742\n",
      "Test score is 0.28383167220376515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "69fe9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained the model on a training set and evaluated its performance on both the training and test sets,calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "27f5a071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.6912751677852349\n",
      "Test score is 0.5846153846153846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=10,p=2)\n",
    "knc.fit(X_train,Y_train)\n",
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cf657d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0acfb156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.703250373997008\n",
      "Test score is 0.26976744186046514\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b1903d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained the model on a training set and evaluated its performance on both the training and test sets,calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "42cf5f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.697986577181208\n",
      "Test score is 0.5846153846153846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=10,p=1)\n",
    "knc.fit(X_train,Y_train)\n",
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "61bb8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9d5a90ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7359649122807017\n",
      "Test score is 0.2791805094130676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "85db6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained the model on a training set and evaluated its performance on both the training and test sets,calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "647100c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7516778523489933\n",
      "Test score is 0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=4,p=2)\n",
    "knc.fit(X_train,Y_train)\n",
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a5d6d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "19973ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7908948728410172\n",
      "Test score is 0.29778516057585824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "734e370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained the model on a training set and evaluated its performance on both the training and test sets,calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "19501dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.7785234899328859\n",
      "Test score is 0.6307692307692307\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=4,p=1)\n",
    "knc.fit(X_train,Y_train)\n",
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "12ddecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "708afcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.8119135046919623\n",
      "Test score is 0.3165005537098561\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "80acd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained the model on a training set and evaluated its performance on both the training and test sets,calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e0e7d47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.8187919463087249\n",
      "Test score is 0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=2,p=2)\n",
    "knc.fit(X_train,Y_train)\n",
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "41da671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7473ee65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.8411328709370325\n",
      "Test score is 0.4141749723145072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "17def8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained the model on a training set and evaluated its performance on both the training and test sets,calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9c403aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.8322147651006712\n",
      "Test score is 0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=2,p=1)\n",
    "knc.fit(X_train,Y_train)\n",
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "012d9798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4c8ec95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 0.8504827961376309\n",
      "Test score is 0.43289036544850507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6f6044ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained the model on a training set and evaluated its performance on both the training and test sets,calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "72c471bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 1.0\n",
      "Test score is 0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=1,p=2)\n",
    "knc.fit(X_train,Y_train)\n",
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c08b4644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "708da14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 1.0\n",
      "Test score is 0.34905869324473976\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d84dce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained the model on a training set and evaluated its performance on both the training and test sets,calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bd28252c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 1.0\n",
      "Test score is 0.6615384615384615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=1,p=1)\n",
    "knc.fit(X_train,Y_train)\n",
    "Y_pred_train = knc.predict(X_train)\n",
    "Y_pred_test = knc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training score is',accuracy_score(Y_train,Y_pred_train))\n",
    "print('Test score is',accuracy_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5f242631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the performance of(knc),K-fold cross-validation is a technique where the dataset is split into k subsets valuated k times, each time using a different fold as the test set and the remaining folds as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d4540395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is 1.0\n",
      "Test score is 0.3631229235880399\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "for train_index,test_index in kfold.split(df2):\n",
    "    x_train,x_test = mm_x.iloc[train_index], mm_x.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knc.fit(x_train,y_train)\n",
    "    y_pred_train = knc.predict(x_train)\n",
    "    y_pred_test = knc.predict(x_test)\n",
    "    train_error.append(accuracy_score(y_train,y_pred_train))\n",
    "    test_error.append(accuracy_score(y_test,y_pred_test))\n",
    "    \n",
    "print('Training score is',np.mean(train_error))\n",
    "print('Test score is',np.mean(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2298ed",
   "metadata": {},
   "source": [
    "# For normal data the accuracy score is high for n_neighbors = 3 and p =2 and p = 1 the accuracy score is 0.83 and \n",
    "# for n_neighbors = 5 and for p = 2 the train accuracy score is 0.73 and test accuracy score is 0.65 and for p=1 the train \n",
    "# accuracuy score is 0.72 and test accuracy score is 0.65, As the n_neighbors is high and acccuracy score is low.\n",
    "# And by using kfold, we are getting better results for train whereas there is no improvement in test results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
